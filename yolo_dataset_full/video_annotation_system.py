#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜å°”å¤«çƒæ†æ£€æµ‹è§†é¢‘æ ‡æ³¨ç³»ç»Ÿ
ä»è§†é¢‘ä¸­æå–å¸§å¹¶è¿›è¡Œäº¤äº’å¼æ ‡æ³¨ï¼Œè¾“å‡ºYOLOæ ¼å¼æ•°æ®é›†
"""

import cv2
import numpy as np
import json
import os
from pathlib import Path
import argparse
from datetime import datetime
import hashlib
import shutil

# æ ‡æ³¨æ¨¡å¼é…ç½®
ANNOTATION_MODES = {
    'bbox': 'è½´å¯¹é½è¾¹ç•Œæ¡†',
    'rotated_bbox': 'æ—‹è½¬è¾¹ç•Œæ¡†', 
    'line': 'çº¿æ®µæ ‡æ³¨',
    'polygon': 'å¤šè¾¹å½¢æ ‡æ³¨'
}

# çƒæ†å®½åº¦é…ç½®
MIN_CLUB_WIDTH_RATIO = 0.002  # æœ€å°å®½åº¦æ¯”ä¾‹
MAX_CLUB_WIDTH_RATIO = 0.008  # æœ€å¤§å®½åº¦æ¯”ä¾‹

# å±å¹•é€‚é…é…ç½®
MAX_DISPLAY_WIDTH = 1200
MAX_DISPLAY_HEIGHT = 800

# å›ºå®šè·¯å¾„é…ç½®
VIDEO_INPUT_DIR = Path(r"C:\Users\Administrator\Desktop\AIGolf\videos")  # è§†é¢‘è¾“å…¥ç›®å½•
DATASET_OUTPUT_DIR = Path(r"C:\Users\Administrator\Desktop\AIGolf\dataset")  # æ•°æ®é›†è¾“å‡ºç›®å½•ï¼ˆè®­ç»ƒè„šæœ¬çš„è¾“å…¥ï¼‰

class VideoAnnotationSystem:
    """è§†é¢‘æ ‡æ³¨ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, mode='rotated_bbox', frame_interval=10, max_frames_per_video=50):
        """
        åˆå§‹åŒ–è§†é¢‘æ ‡æ³¨ç³»ç»Ÿ
        
        Args:
            mode: æ ‡æ³¨æ¨¡å¼ ('bbox', 'rotated_bbox', 'line', 'polygon')
            frame_interval: å¸§é—´éš”ï¼ˆæ¯éš”å¤šå°‘å¸§æå–ä¸€å¸§ï¼‰
            max_frames_per_video: æ¯ä¸ªè§†é¢‘æœ€å¤§æå–å¸§æ•°
        """
        self.mode = mode
        self.frame_interval = frame_interval
        self.max_frames_per_video = max_frames_per_video
        
        # è·¯å¾„é…ç½®
        self.video_input_dir = VIDEO_INPUT_DIR
        self.dataset_output_dir = DATASET_OUTPUT_DIR
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self._ensure_output_directories()
        
        # æ ‡æ³¨çŠ¶æ€
        self.current_image = None
        self.original_image = None
        self.display_image = None
        self.scale_factor = 1.0
        
        # æ ‡æ³¨æ•°æ®
        self.club_points = []
        self.current_frame_info = None
        
        # æ§åˆ¶æ ‡å¿—
        self.drawing = False
        self.finish_annotation = False
        self.skip_image = False
        
        print(f"ğŸ¯ è§†é¢‘æ ‡æ³¨ç³»ç»Ÿå·²åˆå§‹åŒ–")
        print(f"ğŸ“ æ ‡æ³¨æ¨¡å¼: {ANNOTATION_MODES.get(mode, mode)}")
        print(f"ğŸ“ è§†é¢‘è¾“å…¥ç›®å½•: {self.video_input_dir.absolute()}")
        print(f"ğŸ“ æ•°æ®é›†è¾“å‡ºç›®å½•: {self.dataset_output_dir.absolute()}")
        print(f"âš™ï¸ å¸§é—´éš”: {frame_interval}, æœ€å¤§å¸§æ•°: {max_frames_per_video}")
    
    def _ensure_output_directories(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•ç»“æ„å­˜åœ¨"""
        directories = [
            self.dataset_output_dir / "images",
            self.dataset_output_dir / "annotations",
            self.dataset_output_dir / "processed_videos"  # è®°å½•å·²å¤„ç†çš„è§†é¢‘
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    def process_videos(self):
        """å¤„ç†è§†é¢‘ç›®å½•ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
        if not self.video_input_dir.exists():
            print(f"âŒ è§†é¢‘è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.video_input_dir}")
            print("è¯·ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶åŒ…å«è§†é¢‘æ–‡ä»¶")
            return
        
        # è·å–æ‰€æœ‰è§†é¢‘æ–‡ä»¶
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
        video_files = []
        for ext in video_extensions:
            video_files.extend(list(self.video_input_dir.glob(f"*{ext}")))
            video_files.extend(list(self.video_input_dir.glob(f"*{ext.upper()}")))
        
        if not video_files:
            print(f"âŒ åœ¨ç›®å½• {self.video_input_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            print("æ”¯æŒçš„è§†é¢‘æ ¼å¼: .mp4, .avi, .mov, .mkv, .wmv")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # æ£€æŸ¥å·²å¤„ç†çš„è§†é¢‘
        processed_videos = self._get_processed_videos()
        
        total_frames_extracted = 0
        total_frames_annotated = 0
        
        for i, video_path in enumerate(video_files):
            print(f"\nğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {video_path.name}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            video_hash = self._calculate_video_hash(video_path)
            if video_hash in processed_videos:
                print(f"â­ï¸ è§†é¢‘å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {video_path.name}")
                continue
            
            # æå–å¸§å¹¶æ ‡æ³¨
            extracted, annotated = self._process_single_video(video_path)
            total_frames_extracted += extracted
            total_frames_annotated += annotated
            
            # è®°å½•å·²å¤„ç†çš„è§†é¢‘
            self._mark_video_as_processed(video_path, video_hash, extracted, annotated)
        
        print(f"\nğŸ‰ è§†é¢‘å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡æå–å¸§æ•°: {total_frames_extracted}")
        print(f"ğŸ“Š æ€»è®¡æ ‡æ³¨å¸§æ•°: {total_frames_annotated}")
        print(f"ğŸ“ æ•°æ®é›†ä¿å­˜åœ¨: {self.dataset_output_dir}")
    
    def _process_single_video(self, video_path):
        """å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶"""
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†è§†é¢‘: {video_path.name}")
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return 0, 0
        
        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.2f} FPS")
        
        # è®¡ç®—è¦æå–çš„å¸§
        frame_indices = self._calculate_frame_indices(total_frames)
        print(f"ğŸ“‹ å°†æå– {len(frame_indices)} å¸§è¿›è¡Œæ ‡æ³¨")
        
        extracted_count = 0
        annotated_count = 0
        
        # æ˜¾ç¤ºæ“ä½œè¯´æ˜
        self._show_video_instructions(video_path)
        
        for frame_idx in frame_indices:
            # è·³è½¬åˆ°æŒ‡å®šå¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if not ret:
                print(f"âš ï¸ æ— æ³•è¯»å–ç¬¬ {frame_idx} å¸§")
                continue
            
            extracted_count += 1
            
            # å‡†å¤‡å¸§ä¿¡æ¯
            self.current_frame_info = {
                'video_name': video_path.stem,
                'frame_index': frame_idx,
                'timestamp': frame_idx / fps if fps > 0 else 0,
                'video_path': str(video_path)
            }
            
            # æ ‡æ³¨å½“å‰å¸§
            result = self._annotate_frame(frame, frame_idx, len(frame_indices))
            
            if result:
                # ä¿å­˜å¸§å’Œæ ‡æ³¨
                self._save_frame_and_annotation(frame, result)
                annotated_count += 1
                print(f"âœ… å·²æ ‡æ³¨å¸§ {frame_idx}")
            else:
                print(f"â­ï¸ è·³è¿‡å¸§ {frame_idx}")
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: æå– {extracted_count} å¸§, æ ‡æ³¨ {annotated_count} å¸§")
        return extracted_count, annotated_count
    
    def _calculate_frame_indices(self, total_frames):
        """è®¡ç®—è¦æå–çš„å¸§ç´¢å¼•"""
        # æ ¹æ®å¸§é—´éš”è®¡ç®—
        frame_indices = list(range(0, total_frames, self.frame_interval))
        
        # é™åˆ¶æœ€å¤§å¸§æ•°
        if len(frame_indices) > self.max_frames_per_video:
            # å‡åŒ€åˆ†å¸ƒé€‰æ‹©å¸§
            step = len(frame_indices) / self.max_frames_per_video
            frame_indices = [frame_indices[int(i * step)] for i in range(self.max_frames_per_video)]
        
        return frame_indices
    
    def _annotate_frame(self, frame, frame_idx, total_frames):
        """æ ‡æ³¨å•ä¸ªå¸§"""
        # é‡ç½®æ ‡æ³¨çŠ¶æ€
        self.club_points = []
        self.finish_annotation = False
        self.skip_image = False
        
        # è®¾ç½®å½“å‰å›¾åƒ
        self.original_image = frame.copy()
        self.display_image, self.scale_factor = self._resize_for_display(frame)
        self.current_image = self.display_image.copy()
        
        # åˆ›å»ºçª—å£å’Œè®¾ç½®é¼ æ ‡å›è°ƒ
        window_name = f'æ ‡æ³¨å¸§ {frame_idx} ({total_frames} å¸§æ€»è®¡)'
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        cv2.setMouseCallback(window_name, self._mouse_callback)
        
        # æ˜¾ç¤ºå½“å‰å¸§ä¿¡æ¯
        self._show_frame_info(frame_idx, total_frames)
        
        # æ ‡æ³¨å¾ªç¯
        while True:
            cv2.imshow(window_name, self.display_image)
            key = cv2.waitKey(1) & 0xFF
            
            # ESCé”®å¤„ç†
            if key == 27:  # ESC
                if len(self.club_points) == 0:
                    self.skip_image = True
                else:
                    self.finish_annotation = True
            
            # å®Œæˆæ ‡æ³¨æˆ–è·³è¿‡
            if self.finish_annotation or self.skip_image:
                break
        
        cv2.destroyWindow(window_name)
        
        # è¿”å›æ ‡æ³¨ç»“æœ
        if self.skip_image:
            return None
        
        if len(self.club_points) >= 2:
            return self._process_annotation_result()
        
        return None
    
    def _mouse_callback(self, event, x, y, flags, param):
        """é¼ æ ‡äº‹ä»¶å›è°ƒå‡½æ•°"""
        if event == cv2.EVENT_LBUTTONDOWN:
            self.drawing = True
            
            # å°†æ˜¾ç¤ºåæ ‡è½¬æ¢ä¸ºåŸå§‹å›¾åƒåæ ‡
            original_point = self._scale_point_to_original((x, y))
            self.club_points.append(original_point)
            
            # åœ¨æ˜¾ç¤ºå›¾åƒä¸Šç»˜åˆ¶ç‚¹
            cv2.circle(self.display_image, (x, y), max(1, int(3 * self.scale_factor)), (0, 255, 0), -1)
            
            print(f"æ ‡æ³¨ç‚¹ {len(self.club_points)}: æ˜¾ç¤ºåæ ‡({x}, {y}) -> åŸå§‹åæ ‡{original_point}")
            
            # å¦‚æœå·²æœ‰ä¸¤ä¸ªç‚¹ï¼Œç»˜åˆ¶é¢„è§ˆå¹¶å®Œæˆæ ‡æ³¨
            if len(self.club_points) == 2:
                self._draw_annotation_preview()
                self.finish_annotation = True
    
    def _draw_annotation_preview(self):
        """ç»˜åˆ¶æ ‡æ³¨é¢„è§ˆ"""
        if len(self.club_points) < 2:
            return
        
        # é‡æ–°ç»˜åˆ¶æ˜¾ç¤ºå›¾åƒ
        self.display_image = self._resize_for_display(self.original_image)[0]
        
        # ç»˜åˆ¶æ ‡æ³¨ç‚¹
        for i, point in enumerate(self.club_points):
            display_point = self._scale_point_to_display(point)
            cv2.circle(self.display_image, display_point, max(1, int(3 * self.scale_factor)), (0, 255, 0), -1)
            cv2.putText(self.display_image, f"{i+1}", 
                       (display_point[0] + 5, display_point[1] - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # ç»˜åˆ¶è¿çº¿
        if len(self.club_points) >= 2:
            point1_display = self._scale_point_to_display(self.club_points[0])
            point2_display = self._scale_point_to_display(self.club_points[1])
            cv2.line(self.display_image, point1_display, point2_display, (0, 255, 0), max(1, int(2 * self.scale_factor)))
        
        # æ ¹æ®æ¨¡å¼ç»˜åˆ¶ä¸åŒçš„æ ‡æ³¨å½¢çŠ¶
        if self.mode in ['bbox', 'rotated_bbox', 'polygon']:
            self._draw_bounding_shape()
        
        # æ˜¾ç¤ºä¿¡æ¯
        self._draw_annotation_info()
    
    def _draw_bounding_shape(self):
        """æ ¹æ®æ¨¡å¼ç»˜åˆ¶è¾¹ç•Œå½¢çŠ¶"""
        if len(self.club_points) < 2:
            return
        
        point1 = np.array(self.club_points[0])
        point2 = np.array(self.club_points[1])
        
        # è®¡ç®—çƒæ†ä¿¡æ¯
        direction = point2 - point1
        length = np.linalg.norm(direction)
        
        if length == 0:
            return
        
        # è®¡ç®—è‡ªé€‚åº”å®½åº¦
        img_height, img_width = self.original_image.shape[:2]
        club_width = self._calculate_adaptive_width(img_width, img_height, length)
        
        if self.mode == 'rotated_bbox' or self.mode == 'polygon':
            # è®¡ç®—æ—‹è½¬è¾¹ç•Œæ¡†
            corners = self._calculate_rotated_bbox(point1, point2, club_width)
            if corners:
                # è½¬æ¢ä¸ºæ˜¾ç¤ºåæ ‡å¹¶ç»˜åˆ¶
                display_corners = [self._scale_point_to_display(corner) for corner in corners]
                pts = np.array(display_corners, np.int32)
                
                if self.mode == 'polygon':
                    cv2.fillPoly(self.display_image, [pts], (0, 255, 255, 100))
                
                cv2.polylines(self.display_image, [pts], True, (255, 0, 0), max(1, int(2 * self.scale_factor)))
        
        elif self.mode == 'bbox':
            # è®¡ç®—è½´å¯¹é½è¾¹ç•Œæ¡†
            bbox = self._calculate_axis_aligned_bbox(point1, point2, club_width, img_width, img_height)
            if bbox:
                x, y, w, h = bbox
                # è½¬æ¢ä¸ºæ˜¾ç¤ºåæ ‡
                display_x = int(x * self.scale_factor)
                display_y = int(y * self.scale_factor)
                display_w = int(w * self.scale_factor)
                display_h = int(h * self.scale_factor)
                
                cv2.rectangle(self.display_image, (display_x, display_y), 
                            (display_x + display_w, display_y + display_h), 
                            (0, 0, 255), max(1, int(2 * self.scale_factor)))
    
    def _draw_annotation_info(self):
        """ç»˜åˆ¶æ ‡æ³¨ä¿¡æ¯"""
        if len(self.club_points) < 2:
            return
        
        point1 = np.array(self.club_points[0])
        point2 = np.array(self.club_points[1])
        direction = point2 - point1
        length = np.linalg.norm(direction)
        angle = np.degrees(np.arctan2(direction[1], direction[0]))
        
        info_texts = [
            f"è§†é¢‘: {self.current_frame_info['video_name']}",
            f"å¸§: {self.current_frame_info['frame_index']}",
            f"æ¨¡å¼: {ANNOTATION_MODES.get(self.mode, self.mode)}",
            f"é•¿åº¦: {length:.1f}px",
            f"è§’åº¦: {angle:.1f}Â°"
        ]
        
        # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºä¿¡æ¯
        for i, text in enumerate(info_texts):
            y_pos = 30 + i * 25
            cv2.putText(self.display_image, text, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(self.display_image, text, (9, y_pos - 1), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
    
    def _process_annotation_result(self):
        """å¤„ç†æ ‡æ³¨ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„æ ‡æ³¨æ•°æ®"""
        if len(self.club_points) < 2:
            return None
        
        point1 = np.array(self.club_points[0])
        point2 = np.array(self.club_points[1])
        direction = point2 - point1
        length = np.linalg.norm(direction)
        
        if length == 0:
            return None
        
        img_height, img_width = self.original_image.shape[:2]
        club_width = self._calculate_adaptive_width(img_width, img_height, length)
        
        result = {
            'mode': self.mode,
            'points': self.club_points[:2],
            'length': length,
            'angle': np.degrees(np.arctan2(direction[1], direction[0])),
            'club_width': club_width,
            'image_size': (img_width, img_height),
            'frame_info': self.current_frame_info.copy()
        }
        
        # æ ¹æ®æ¨¡å¼è®¡ç®—ä¸åŒçš„å‡ ä½•ä¿¡æ¯
        if self.mode == 'line':
            result['center_point'] = ((point1[0] + point2[0]) / 2, (point1[1] + point2[1]) / 2)
        
        elif self.mode in ['bbox', 'rotated_bbox', 'polygon']:
            if self.mode == 'rotated_bbox' or self.mode == 'polygon':
                corners = self._calculate_rotated_bbox(point1, point2, club_width)
                result['rotated_corners'] = corners
                result['rotated_area'] = self._calculate_polygon_area(corners) if corners else 0
            
            # è®¡ç®—è½´å¯¹é½è¾¹ç•Œæ¡†ï¼ˆç”¨äºYOLOæ ¼å¼ï¼‰
            bbox = self._calculate_axis_aligned_bbox(point1, point2, club_width, img_width, img_height)
            result['bbox'] = bbox
        
        return result
    
    def _save_frame_and_annotation(self, frame, annotation_data):
        """ä¿å­˜å¸§å›¾åƒå’Œæ ‡æ³¨æ•°æ®"""
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        video_name = self.current_frame_info['video_name']
        frame_idx = self.current_frame_info['frame_index']
        
        filename = f"{video_name}_frame_{frame_idx:06d}_{timestamp}"
        
        # ä¿å­˜å›¾åƒ
        image_path = self.dataset_output_dir / "images" / f"{filename}.jpg"
        cv2.imwrite(str(image_path), frame)
        
        # ä¿å­˜YOLOæ ¼å¼æ ‡æ³¨
        if 'bbox' in annotation_data:
            self._save_yolo_annotation(filename, annotation_data)
        
        # ä¿å­˜è¯¦ç»†æ ‡æ³¨ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰
        self._save_detailed_annotation(filename, annotation_data)
    
    def _save_yolo_annotation(self, filename, annotation_data):
        """ä¿å­˜YOLOæ ¼å¼æ ‡æ³¨"""
        annotation_path = self.dataset_output_dir / "annotations" / f"{filename}.txt"
        
        if 'bbox' in annotation_data:
            x, y, w, h = annotation_data['bbox']
            img_width, img_height = annotation_data['image_size']
            
            # è½¬æ¢ä¸ºYOLOæ ¼å¼ (å½’ä¸€åŒ–çš„ä¸­å¿ƒç‚¹åæ ‡å’Œå®½é«˜)
            x_center = (x + w / 2) / img_width
            y_center = (y + h / 2) / img_height
            norm_width = w / img_width
            norm_height = h / img_height
            
            # ç±»åˆ«ID (çƒæ†ä¸º0)
            class_id = 0
            
            with open(annotation_path, 'w') as f:
                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\n")
    
    def _save_detailed_annotation(self, filename, annotation_data):
        """ä¿å­˜è¯¦ç»†æ ‡æ³¨ä¿¡æ¯"""
        detail_path = self.dataset_output_dir / "annotations" / f"{filename}_detail.json"
        
        with open(detail_path, 'w', encoding='utf-8') as f:
            json.dump(annotation_data, f, indent=2, ensure_ascii=False)
    
    def _resize_for_display(self, image):
        """å°†å›¾åƒç¼©æ”¾åˆ°é€‚åˆæ˜¾ç¤ºçš„å°ºå¯¸"""
        height, width = image.shape[:2]
        scale = min(MAX_DISPLAY_WIDTH / width, MAX_DISPLAY_HEIGHT / height, 1.0)
        
        if scale < 1.0:
            new_width = int(width * scale)
            new_height = int(height * scale)
            resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
            return resized, scale
        else:
            return image.copy(), 1.0
    
    def _scale_point_to_original(self, point):
        """å°†æ˜¾ç¤ºåæ ‡è½¬æ¢ä¸ºåŸå§‹å›¾åƒåæ ‡"""
        x, y = point
        return (int(x / self.scale_factor), int(y / self.scale_factor))
    
    def _scale_point_to_display(self, point):
        """å°†åŸå§‹å›¾åƒåæ ‡è½¬æ¢ä¸ºæ˜¾ç¤ºåæ ‡"""
        x, y = point
        return (int(x * self.scale_factor), int(y * self.scale_factor))
    
    def _calculate_adaptive_width(self, img_width, img_height, club_length):
        """è®¡ç®—è‡ªé€‚åº”çƒæ†å®½åº¦"""
        base_width = img_width * MIN_CLUB_WIDTH_RATIO
        max_width = img_width * MAX_CLUB_WIDTH_RATIO
        
        # æ ¹æ®çƒæ†é•¿åº¦è°ƒæ•´
        if club_length < img_width * 0.1:
            width_factor = 0.8
        elif club_length < img_width * 0.3:
            width_factor = 1.0
        else:
            width_factor = 1.2
        
        adaptive_width = base_width * width_factor
        return max(min(adaptive_width, max_width), 3)
    
    def _calculate_rotated_bbox(self, point1, point2, width):
        """è®¡ç®—æ—‹è½¬è¾¹ç•Œæ¡†çš„å››ä¸ªè§’ç‚¹"""
        point1 = np.array(point1)
        point2 = np.array(point2)
        
        direction = point2 - point1
        length = np.linalg.norm(direction)
        
        if length == 0:
            return None
        
        direction_norm = direction / length
        perpendicular = np.array([-direction_norm[1], direction_norm[0]])
        half_width = width / 2
        
        corners = [
            point1 + perpendicular * half_width,  # å·¦ä¸Š
            point2 + perpendicular * half_width,  # å³ä¸Š
            point2 - perpendicular * half_width,  # å³ä¸‹
            point1 - perpendicular * half_width   # å·¦ä¸‹
        ]
        
        return [corner.tolist() for corner in corners]
    
    def _calculate_axis_aligned_bbox(self, point1, point2, width, img_width, img_height):
        """è®¡ç®—è½´å¯¹é½è¾¹ç•Œæ¡†"""
        corners = self._calculate_rotated_bbox(point1, point2, width)
        if not corners:
            return None
        
        corners_array = np.array(corners)
        x_min = max(0, np.min(corners_array[:, 0]))
        x_max = min(img_width, np.max(corners_array[:, 0]))
        y_min = max(0, np.min(corners_array[:, 1]))
        y_max = min(img_height, np.max(corners_array[:, 1]))
        
        return [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]
    
    def _calculate_polygon_area(self, corners):
        """è®¡ç®—å¤šè¾¹å½¢é¢ç§¯"""
        if not corners or len(corners) < 3:
            return 0
        
        corners_array = np.array(corners)
        x = corners_array[:, 0]
        y = corners_array[:, 1]
        
        return 0.5 * abs(sum(x[i] * y[(i + 1) % len(corners)] - x[(i + 1) % len(corners)] * y[i] 
                            for i in range(len(corners))))
    
    def _calculate_video_hash(self, video_path):
        """è®¡ç®—è§†é¢‘æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(video_path, "rb") as f:
            # åªè¯»å–æ–‡ä»¶çš„å¼€å¤´å’Œç»“å°¾éƒ¨åˆ†æ¥è®¡ç®—å“ˆå¸Œï¼ˆæé«˜é€Ÿåº¦ï¼‰
            chunk = f.read(8192)
            hash_md5.update(chunk)
            
            # è·³åˆ°æ–‡ä»¶ä¸­é—´
            f.seek(f.seek(0, 2) // 2)  # æ–‡ä»¶å¤§å°çš„ä¸€åŠ
            chunk = f.read(8192)
            hash_md5.update(chunk)
            
            # è·³åˆ°æ–‡ä»¶æœ«å°¾
            f.seek(-8192, 2)
            chunk = f.read(8192)
            hash_md5.update(chunk)
        
        return hash_md5.hexdigest()
    
    def _get_processed_videos(self):
        """è·å–å·²å¤„ç†çš„è§†é¢‘åˆ—è¡¨"""
        processed_file = self.dataset_output_dir / "processed_videos" / "processed_list.json"
        
        if processed_file.exists():
            with open(processed_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('processed_hashes', []))
        
        return set()
    
    def _mark_video_as_processed(self, video_path, video_hash, extracted_count, annotated_count):
        """æ ‡è®°è§†é¢‘ä¸ºå·²å¤„ç†"""
        processed_file = self.dataset_output_dir / "processed_videos" / "processed_list.json"
        
        # è¯»å–ç°æœ‰æ•°æ®
        if processed_file.exists():
            with open(processed_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'processed_videos': [], 'processed_hashes': []}
        
        # æ·»åŠ æ–°è®°å½•
        video_record = {
            'video_name': video_path.name,
            'video_path': str(video_path),
            'video_hash': video_hash,
            'processed_time': datetime.now().isoformat(),
            'extracted_frames': extracted_count,
            'annotated_frames': annotated_count,
            'annotation_mode': self.mode
        }
        
        data['processed_videos'].append(video_record)
        data['processed_hashes'].append(video_hash)
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(processed_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def _show_video_instructions(self, video_path):
        """æ˜¾ç¤ºè§†é¢‘å¤„ç†è¯´æ˜"""
        print(f"\nğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {video_path.name}")
        print("ğŸ–±ï¸  æ“ä½œè¯´æ˜:")
        print("   - å·¦é”®ç‚¹å‡»çƒæ†ä¸¤ç«¯è¿›è¡Œæ ‡æ³¨")
        print("   - ESCé”®è·³è¿‡å½“å‰å¸§")
        print("   - æ ‡æ³¨å®Œæˆåè‡ªåŠ¨è¿›å…¥ä¸‹ä¸€å¸§")
    
    def _show_frame_info(self, frame_idx, total_frames):
        """æ˜¾ç¤ºå½“å‰å¸§ä¿¡æ¯"""
        print(f"ğŸ“¸ æ ‡æ³¨å¸§ {frame_idx} (å…± {total_frames} å¸§)")
        print(f"ğŸ¬ è§†é¢‘: {self.current_frame_info['video_name']}")
        print(f"â±ï¸  æ—¶é—´æˆ³: {self.current_frame_info['timestamp']:.2f}s")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é«˜å°”å¤«çƒæ†æ£€æµ‹è§†é¢‘æ ‡æ³¨ç³»ç»Ÿ")
    parser.add_argument("--mode", "-m", choices=list(ANNOTATION_MODES.keys()), 
                       default="rotated_bbox", help="æ ‡æ³¨æ¨¡å¼")
    parser.add_argument("--frame_interval", "-i", type=int, default=10, 
                       help="å¸§é—´éš”ï¼ˆæ¯éš”å¤šå°‘å¸§æå–ä¸€å¸§ï¼‰")
    parser.add_argument("--max_frames", "-f", type=int, default=50, 
                       help="æ¯ä¸ªè§†é¢‘æœ€å¤§æå–å¸§æ•°")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè§†é¢‘æ ‡æ³¨ç³»ç»Ÿ
        system = VideoAnnotationSystem(
            mode=args.mode,
            frame_interval=args.frame_interval,
            max_frames_per_video=args.max_frames
        )
        
        # å¼€å§‹å¤„ç†è§†é¢‘
        system.process_videos()
        
    except Exception as e:
        print(f"âŒ è§†é¢‘æ ‡æ³¨è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 