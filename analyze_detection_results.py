#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æµ‹ç»“æœåˆ†æè„šæœ¬
åˆ†ææ¨¡å‹æ£€æµ‹æ€§èƒ½å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse

def analyze_detection_results(results_file):
    """åˆ†ææ£€æµ‹ç»“æœ"""
    
    # è¯»å–ç»“æœæ–‡ä»¶
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 60)
    print("é«˜å°”å¤«çƒæ†æ£€æµ‹ç»“æœåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    # åŸºæœ¬ä¿¡æ¯
    video_info = data['video_info']
    detection_summary = data['detection_summary']
    frame_results = data['frame_results']
    
    print(f"\nğŸ“¹ è§†é¢‘ä¿¡æ¯:")
    print(f"  æ–‡ä»¶è·¯å¾„: {data['video_path']}")
    print(f"  åˆ†è¾¨ç‡: {video_info['width']}x{video_info['height']}")
    print(f"  å¸§ç‡: {video_info['fps']} FPS")
    print(f"  æ€»å¸§æ•°: {video_info['total_frames']}")
    
    print(f"\nğŸ¯ æ£€æµ‹æ¦‚è§ˆ:")
    print(f"  æ€»æ£€æµ‹æ•°: {detection_summary['total_detections']}")
    print(f"  æœ‰æ£€æµ‹çš„å¸§æ•°: {detection_summary['frames_with_detections']}/{video_info['total_frames']}")
    print(f"  æ£€æµ‹ç‡: {detection_summary['detection_rate']*100:.1f}%")
    
    # è¯¦ç»†åˆ†æ
    detections_per_frame = [result['detections'] for result in frame_results]
    confidences_all = []
    
    for result in frame_results:
        confidences_all.extend(result['confidences'])
    
    print(f"\nğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
    print(f"  å¹³å‡æ¯å¸§æ£€æµ‹æ•°: {np.mean(detections_per_frame):.2f}")
    print(f"  æœ€å¤§å•å¸§æ£€æµ‹æ•°: {max(detections_per_frame)}")
    print(f"  æœ€å°å•å¸§æ£€æµ‹æ•°: {min(detections_per_frame)}")
    
    if confidences_all:
        print(f"\nğŸ² ç½®ä¿¡åº¦åˆ†æ:")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences_all):.3f}")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {max(confidences_all):.3f}")
        print(f"  æœ€ä½ç½®ä¿¡åº¦: {min(confidences_all):.3f}")
        print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(confidences_all):.3f}")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_ranges = [
            (0.05, 0.1, "æä½"),
            (0.1, 0.2, "ä½"),
            (0.2, 0.3, "ä¸­ä½"),
            (0.3, 0.5, "ä¸­ç­‰"),
            (0.5, 0.7, "é«˜"),
            (0.7, 1.0, "æé«˜")
        ]
        
        print(f"\nğŸ“ˆ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        for min_conf, max_conf, label in confidence_ranges:
            count = sum(1 for c in confidences_all if min_conf <= c < max_conf)
            percentage = count / len(confidences_all) * 100
            print(f"  {label} ({min_conf:.1f}-{max_conf:.1f}): {count} ({percentage:.1f}%)")
    
    # æ—¶é—´åºåˆ—åˆ†æ
    print(f"\nâ±ï¸ æ—¶é—´åºåˆ—åˆ†æ:")
    
    # æ£€æµ‹è¿ç»­æ€§
    consecutive_detections = 0
    max_consecutive = 0
    current_consecutive = 0
    
    for result in frame_results:
        if result['detections'] > 0:
            current_consecutive += 1
            max_consecutive = max(max_consecutive, current_consecutive)
        else:
            current_consecutive = 0
    
    print(f"  æœ€é•¿è¿ç»­æ£€æµ‹: {max_consecutive} å¸§")
    
    # æ£€æµ‹ç¨³å®šæ€§ï¼ˆç›¸é‚»å¸§æ£€æµ‹æ•°å˜åŒ–ï¼‰
    detection_changes = []
    for i in range(1, len(detections_per_frame)):
        change = abs(detections_per_frame[i] - detections_per_frame[i-1])
        detection_changes.append(change)
    
    if detection_changes:
        print(f"  æ£€æµ‹ç¨³å®šæ€§ (å¹³å‡å˜åŒ–): {np.mean(detection_changes):.2f}")
        print(f"  æœ€å¤§å¸§é—´å˜åŒ–: {max(detection_changes)}")
    
    # æ€§èƒ½è¯„ä¼°
    print(f"\nğŸ† æ€§èƒ½è¯„ä¼°:")
    
    # æ ¹æ®æ£€æµ‹ç‡è¯„ä¼°
    detection_rate = detection_summary['detection_rate']
    if detection_rate >= 0.9:
        performance_level = "ä¼˜ç§€"
        performance_color = "ğŸŸ¢"
    elif detection_rate >= 0.7:
        performance_level = "è‰¯å¥½"
        performance_color = "ğŸŸ¡"
    elif detection_rate >= 0.5:
        performance_level = "ä¸€èˆ¬"
        performance_color = "ğŸŸ "
    else:
        performance_level = "éœ€è¦æ”¹è¿›"
        performance_color = "ğŸ”´"
    
    print(f"  æ€»ä½“æ€§èƒ½: {performance_color} {performance_level}")
    
    # æ ¹æ®ç½®ä¿¡åº¦è¯„ä¼°
    if confidences_all:
        avg_confidence = np.mean(confidences_all)
        if avg_confidence >= 0.5:
            confidence_level = "é«˜ç½®ä¿¡åº¦"
            confidence_color = "ğŸŸ¢"
        elif avg_confidence >= 0.2:
            confidence_level = "ä¸­ç­‰ç½®ä¿¡åº¦"
            confidence_color = "ğŸŸ¡"
        else:
            confidence_level = "ä½ç½®ä¿¡åº¦"
            confidence_color = "ğŸŸ "
        
        print(f"  ç½®ä¿¡åº¦æ°´å¹³: {confidence_color} {confidence_level}")
    
    # å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    
    if detection_rate < 0.8:
        print("  â€¢ æ£€æµ‹ç‡åä½ï¼Œå»ºè®®:")
        print("    - å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
        print("    - è°ƒæ•´æ¨¡å‹æ¶æ„")
        print("    - ä¼˜åŒ–æ•°æ®å¢å¼ºç­–ç•¥")
    
    if confidences_all and np.mean(confidences_all) < 0.3:
        print("  â€¢ ç½®ä¿¡åº¦åä½ï¼Œå»ºè®®:")
        print("    - å»¶é•¿è®­ç»ƒæ—¶é—´")
        print("    - è°ƒæ•´å­¦ä¹ ç‡")
        print("    - æ£€æŸ¥æ ‡æ³¨è´¨é‡")
    
    if max(detection_changes) > 3:
        print("  â€¢ æ£€æµ‹ä¸ç¨³å®šï¼Œå»ºè®®:")
        print("    - æ·»åŠ æ—¶åºä¸€è‡´æ€§çº¦æŸ")
        print("    - ä½¿ç”¨è·Ÿè¸ªç®—æ³•å¹³æ»‘ç»“æœ")
        print("    - å¢åŠ è§†é¢‘æ•°æ®è®­ç»ƒ")
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    
    return {
        'detection_rate': detection_rate,
        'avg_confidence': np.mean(confidences_all) if confidences_all else 0,
        'total_detections': detection_summary['total_detections'],
        'performance_level': performance_level
    }

def plot_detection_timeline(results_file, output_path=None):
    """ç»˜åˆ¶æ£€æµ‹æ—¶é—´çº¿å›¾"""
    
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    frame_results = data['frame_results']
    frames = [result['frame'] for result in frame_results]
    detections = [result['detections'] for result in frame_results]
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    avg_confidences = []
    for result in frame_results:
        if result['confidences']:
            avg_confidences.append(np.mean(result['confidences']))
        else:
            avg_confidences.append(0)
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # æ£€æµ‹æ•°é‡æ—¶é—´çº¿
    ax1.plot(frames, detections, 'b-', linewidth=2, label='æ£€æµ‹æ•°é‡')
    ax1.fill_between(frames, detections, alpha=0.3)
    ax1.set_xlabel('å¸§æ•°')
    ax1.set_ylabel('æ£€æµ‹æ•°é‡')
    ax1.set_title('æ¯å¸§æ£€æµ‹æ•°é‡æ—¶é—´çº¿')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # å¹³å‡ç½®ä¿¡åº¦æ—¶é—´çº¿
    ax2.plot(frames, avg_confidences, 'r-', linewidth=2, label='å¹³å‡ç½®ä¿¡åº¦')
    ax2.fill_between(frames, avg_confidences, alpha=0.3, color='red')
    ax2.set_xlabel('å¸§æ•°')
    ax2.set_ylabel('å¹³å‡ç½®ä¿¡åº¦')
    ax2.set_title('æ¯å¸§å¹³å‡ç½®ä¿¡åº¦æ—¶é—´çº¿')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"æ—¶é—´çº¿å›¾å·²ä¿å­˜åˆ°: {output_path}")
    else:
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ£€æµ‹ç»“æœåˆ†æ")
    parser.add_argument("--results", type=str, required=True, help="æ£€æµ‹ç»“æœJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--plot", action="store_true", help="ç”Ÿæˆæ—¶é—´çº¿å›¾è¡¨")
    parser.add_argument("--output", type=str, help="å›¾è¡¨è¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    if not Path(args.results).exists():
        print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.results}")
        return
    
    # åˆ†æç»“æœ
    analysis = analyze_detection_results(args.results)
    
    # ç”Ÿæˆå›¾è¡¨
    if args.plot:
        plot_detection_timeline(args.results, args.output)

if __name__ == "__main__":
    main() 